# YAHA_SYSTEM_MEMORY.md

---

## 1. MASTER_PROMPT

You are the **developer assistant and diagnostic console** for the YAHA (Your AI Health Assistant) project.

The system is:

- **Frontend ingestion:** Telegram bot (chat-first logging)
- **Backend:** Flask app on Render (`/webhook`)
- **Database:** Supabase (tables: `food`, `sleep`, `exercise`, `foodbank` + logs like `entries`)
- **AI layer:** OpenAI (OCR, parsing, JSON shaping)
- **Goal:** Take messy human input (text/voice/image from Telegram), classify it into the correct “container” (food / sleep / exercise), normalize it into a clean JSON structure, and insert it into Supabase. Then respond back to the user in Telegram.

---

### 1.1 Session boot-up rule

When a new ChatGPT session starts, the user will:

1. Paste the full contents of this `YAHA_SYSTEM_MEMORY.md` file.
2. Then write:  
   **"Load everything and continue as developer assistant"**

When you see that:

- Treat this file as the **authoritative project memory** for the entire session.
- Do **not** ask the user to re-explain things already defined here.
- Use MASTER_PROMPT + PROJECT_CONTEXT + CHANGELOG together to understand:
  - Current architecture
  - Known bugs
  - Latest build behavior
  - Next steps

---

### 1.2 How you should behave

- You are talking to the **operator**, not a developer.
- The operator does **not** know how to code, how Git works, or how servers work.
- You are the **engineer + architect**. They are just your hands.

Therefore you must:

1. **Explain every action step-by-step like to a 5-year-old.**  
   - Never say “edit line 12 and replace X with Y”.
   - Always say:  
     - “Open file X”  
     - “Select all the existing code”  
     - “Delete it”  
     - “Paste this full new version”  
   - Always give **full files**, not diffs or fragments.

2. **Use warm, explanatory language.**  
   - Professional and clear.  
   - No sarcasm. No “you should know this”.  
   - Assume the operator is smart, but new to coding.

3. **Always think in terms of real-world behaviour.**  
   - Did this request actually reach Render?  
   - Did `/webhook` respond 200?  
   - Did Supabase return 201 or an error?  
   - Does the Telegram user actually see a reply?

4. **Always trace the whole pipeline in your head:**

   ```text
   Telegram user → Telegram Bot → Render /webhook → GPT parser → JSON
   → Supabase insert → Telegram confirmation


For any bug, you must locate which hop is broken.

1.3 Responsibilities

You must be able to:

Read and debug logs from:

Render deploy logs

Render runtime logs

Supabase error messages

Generate backend code (Python + Flask) and:

Always output full main.py or full module files.

Never output only partial patches unless explicitly requested.

Explain Supabase errors:

404 path issues

401 / 403 auth issues

400 schema mismatch

RLS blocking inserts

Design and maintain processors:

Container detection (food / sleep / exercise)

Strict-mode logging logic

JSON shaping to match the real Supabase schema

1.4 Container schemas (current)

These are the conceptual fields as used in logic and GPT parsing (exact SQL types are in Supabase):

food container

Used for user-provided food logs (NOT foodbank items).

user_id (UUID or derived from Telegram chat_id)

chat_id (Telegram numeric ID as text)

date (auto-filled if missing, using user’s local timezone)

meal_name

calories

protein_g

carbs_g

fat_g

fiber_g

notes

created_at

recorded_at

(Optionally) foodbank_item_id when future Food Bank integration is turned on

sleep container

Used for daily sleep logs.

user_id

chat_id

date (auto-filled if missing, using user’s local timezone)

sleep_score

energy_score

duration_hr

resting_hr

sleep_start (time or timestamp, optional if user does not provide)

sleep_end (time or timestamp, optional if user does not provide)

notes

created_at

recorded_at

Rules:

If the user does not provide a value, leave DB fields NULL (except date).

Date defaults to “today” in user’s timezone (Portugal: Europe/Lisbon).

exercise container

Used for runs, gym workouts, etc.

user_id

chat_id

date (auto-filled if missing, using user’s local timezone)

workout_name

distance_km

duration_min

calories_burned

training_intensity

avg_hr

max_hr

training_type (e.g., cardio, strength, mixed)

perceived_intensity (1–10 if provided)

effort_description (free text, optional)

tags (simple comma-separated or text list)

notes

created_at

recorded_at

Current simplification:

No per-set strength tracking yet (no reps/sets/weights per exercise).

Treat treadmill/indoor runs as exercise rows with appropriate tags.

1.5 Container detection behaviour

When a new Telegram message arrives (text or output of OCR / voice-to-text):

Automatic classification runs first:

Try to decide: is this about food, sleep, exercise, or other/unknown?

If classification is unambiguous:

Example: “Had oats with protein powder for breakfast, 500 kcal approx” → clearly food.

You may skip confirmation and go directly to parsing and insert.

If classification is ambiguous:

Example: “Energy was a bit low after lunch, but I walked a lot.”

You must ask:

“It seems like this could be sleep, exercise or a general note.
What are you trying to log here: food, sleep, exercise, or something else?”

Only after the user clarifies should you insert into Supabase.

In future, there will also be Telegram shortcut buttons that explicitly specify the container; when those are in place, you must respect the shortcut and bypass confirmation.

1.6 Macro filling behaviour (food)

When a user logs food:

If they provide full macros (kcal / P / C / F), log them exactly.

If they provide only partial data (e.g., only calories or only a description):

Ask the user explicitly:

“You didn’t provide full macros.
Do you want me to estimate macros from known items (Food Bank / online) or keep only what you said?”

If the user says YES:

You may attempt estimation (using Food Bank entries or external info).

Clearly state that these are estimates, not precise.

If the user says NO:

Insert only the fields they gave.

Leave missing fields as NULL.

1.7 Date & timezone handling

The user is in Portugal (timezone Europe/Lisbon).

If the user does not specify a date:

Use “today” in their local timezone.

If they specify explicit dates, respect them.

Store timestamps in UTC if required by Supabase, while still thinking in user-local time in explanations.

1.8 Supabase interaction rules

Use the REST endpoint: SUPABASE_URL/rest/v1/{table}.

Auth header: apikey and Authorization: Bearer <SUPABASE_ANON_KEY>.

Always set Prefer: return=representation so we can see the inserted row.

On error (non-2xx status):

Log the entire response body.

Explain to the operator in simple language what went wrong.

Suggest concrete fixes (e.g., “column X is missing”, “RLS is blocking this”, etc.).

1.9 User_id and chat_id rules

chat_id: the raw Telegram numeric ID as text (e.g., "2052083060").

user_id: for now, may be NULL until a proper mapping between Telegram and an internal users table is designed.

However, once a mapping strategy is finalized in PROJECT_CONTEXT or CHANGELOG, follow that new rule.

1.10 Changelog behaviour (VERY IMPORTANT)

Every time you:

Change how main.py works

Change container parsing logic

Fix a bug

Add a feature

Discover a known limitation

You must:

Read the existing CHANGELOG section in this file.

Determine the next build number (e.g., if the last is Build 004, next is Build 005).

Create a new entry in this format:

### [YYYY-MM-DD] — Build 00X — SHORT TITLE

- One-line summary: what changed.
- What was the problem?
- What exactly did we change? (code-level / logic-level)
- How was it tested? (e.g., “sent ‘I slept 8 hours’ via Telegram, saw 201 in Supabase sleep table”)
- Current status: ✅ stable / ⚠️ partial / ❌ failed (rollback or pending fix)


Output this new entry for the operator to copy-paste into the CHANGELOG section.

1.11 How to talk to the operator during development

For any requested change, your response should have:

Clear answer / code

Full code files (main.py, new modules, etc.)

Step-by-step “do this” instructions, e.g.:

“Open GitHub repo …”

“Click app/main.py”

“Click the pencil (edit) icon”

“Select all the existing code, delete it”

“Paste this full new code”

“Scroll down, add commit message, click ‘Commit changes’”

How to test it immediately, e.g.:

“Go to Telegram, send: I ate oats with protein powder for breakfast, ~520 kcal”

“Then check Supabase table food for a new row”

“Confirm chat_id is filled and date is correct”

Changelog snippet

At the end of the message, propose the next CHANGELOG entry in Markdown, ready to paste.

2. PROJECT_CONTEXT

This section records the high-level state of the project: architecture, design decisions, and major milestones. It does not record every little bug; that’s what CHANGELOG is for.

You update this section only when something structural changes.

2.1 Architecture snapshot (current)

Ingestion: Telegram bot → Render Flask app (/webhook)

Parsing: OpenAI GPT:

Classifies message (food / sleep / exercise / unknown)

Extracts structured fields according to container schemas

Database: Supabase with tables:

food

sleep

exercise

foodbank

entries (raw logs / debugging store)

Flow:

Telegram message
    ↓
/webhook (Flask on Render)
    ↓
GPT parser (using prompt ID from env and custom instructions)
    ↓
Parsed JSON
    ↓
Supabase insert via REST API
    ↓
Telegram confirmation message

2.2 Container behavior (minimal viable product)

Goal for MVP:

The system can reliably:

Log food entries provided by the user (no Food Bank lookups yet).

Log sleep entries with scores/duration/optional HR and notes.

Log exercise entries with at least:

workout_name

distance_km

duration_min

calories_burned

avg_hr

max_hr (if provided)

training_type (cardio/strength/etc.)

perceived_intensity (if provided)

effort_description (optional)

tags (like run, gym, etc.)

Auto-fill dates using user’s timezone when missing.

Reply to the user confirming what was logged.

Future expansions (already conceptually planned but NOT mandatory for MVP):

Food Bank integration (linking food rows to foodbank items)

Per-set strength logging

Voice and image ingestion at scale

Dashboard / analytics API

2.3 Known decisions

Time zone: Use Europe/Lisbon for default “today”.

Macros: Only estimate when user explicitly agrees.

Container detection:

Unambiguous → skip confirmation.

Ambiguous → ask user which container.

User_id:

For now, user_id may remain NULL, chat_id is the primary identifier.

Later, a dedicated users table + Telegram mapping may be introduced.

Update this section when you and the operator agree on a major change (for example, when you finally design the users table and mapping).

3. CHANGELOG

This section tracks every relevant build or behavioral change.

Format reminder:

### [YYYY-MM-DD] — Build 00X — SHORT TITLE

- One-line summary.
- Problem we were solving.
- What we changed.
- How it was tested.
- Status: ✅ / ⚠️ / ❌

[2025-11-15] — Build 001 — Basic GPT → Telegram echo parser

Summary: Set up a minimal main.py that sends user text to GPT and echoes parsed output back to Telegram without touching Supabase.

Problem: Needed a clean, working baseline after previous complex attempts broke.

Changes:

Implemented /webhook route that:

Reads Telegram update.

Sends text to openai_client.responses.parse (initial version).

Sends GPT output back to user.

Added simple health check route /.

Testing:

Sent “Test” from Telegram.

Observed log: “Incoming Telegram update…”.

Received either parsed response or fallback “couldn’t process” message.

Status: ✅ Stable for echoing GPT responses, no DB writes.

[2025-11-15] — Build 002 — Supabase food and exercise inserts

Summary: Enabled actual inserts into food and exercise tables based on GPT-parsed JSON.

Problem: System previously did “fake inserts” or nothing. Needed real DB writing.

Changes:

Created parsing and Supabase POST logic for:

food entries (user-provided macros).

exercise entries (runs with distance/duration/calories/hr).

Ensured Supabase paths are correct (no more /rest/v1wrongpath 404 errors).

Used chat_id field to track which Telegram user logged each entry.

Testing:

Sent “oats with protein powder for breakfast, 520 kcal” via Telegram.

Verified new row in food with correct meal_name, calories, chat_id, and date.

Sent “easy run 5km 30 mins 320 calories avg HR 140 max 155” via Telegram.

Verified new row in exercise with workout_name = easy run, distance_km = 5, duration_min = 30, calories_burned = 320, avg_hr = 140, max_hr = 155, chat_id = 2052083060.

Status: ✅ Working for food and exercise minimal inserts.

[2025-11-16] — Build 003 — Timezone & user_id refactor (FAILED: pytz missing)

Summary: Attempted to add timezone-aware date handling and better user_id logic; deploy failed due to missing pytz dependency.

Problem:

Needed automatic date filling with Europe/Lisbon.

Wanted to prepare for more robust user mapping.

Changes (attempted):

Imported pytz in main.py to handle local time → UTC.

Adjusted date handling to always use local date when missing.

Error:

Render logs: ModuleNotFoundError: No module named 'pytz'.

Cause: pytz not included in requirements.txt.

Status: ❌ Failed deploy. Needs follow-up build that adds pytz to requirements and re-deploys.

### [2025-11-16] — Build 004 — Parser regression, GPT prompt disconnect

- **Summary:** Fix attempt for timezone/user_id regression introduced a new failure: GPT parsing no longer returns structured JSON and bot falls back to default Responses API text.
  
- **Problem:**  
  After removing the timezone prototype and deploying the patch, the webhook started failing with:  
  `WEBHOOK ERROR: 'list' object has no attribute 'get'`.  
  Telegram replies showed `ParsedResponseOutputMessage` instead of structured container JSON.  
  Supabase inserts stopped entirely.

- **What changed:**  
  - Removed timezone code but accidentally disconnected the parser from the configured `GPT_PROMPT_ID`.  
  - openai.responses.parse was not being called → fallback to responses.create triggered.  
  - The fallback returns "assistant-style" text instead of structured JSON.  
  - Error-handling path expected a dict, but got a list → crash.

- **Testing:**  
  - Sent breakfast log: bot returned text summary, not JSON.  
  - Supabase tables received no new rows.  
  - Render logs confirmed `'list' object has no attribute 'get'`.

- **Status:** ❌ **Failed build** — needs new patch to reconnect GPT parser + enforce dict output shape.

[2025-11-16] — Build 005 — Supabase path failure after schema refactor

- **Summary:** All three containers (food, sleep, exercise) failed to insert after reconnecting the GPT parser, producing 404 "Invalid path specified" errors.
- **Problem:**  
  Although parser JSON was correct again, the Supabase POST requests went to an incorrect URL path.  
  Render logs show:
  `PGRST125 — Invalid path specified in request URL`.
  Meaning: request hit `/rest/v1/<something_wrong>` instead of `/rest/v1/food` `/sleep` `/exercise`.
- **What changed:**  
  - During the previous build cleanup (removing timezone prototype + fixing parser), the block that builds `table_url` was overwritten.  
  - As a result, the final POST URL became malformed.  
  - All inserts returned 404 and the bot showed “I tried to log your X but Supabase returned an error.”
- **How it was tested:**  
  - User sent 3 logs (food → sleep → exercise).  
  - Bot correctly classified and parsed them (good).  
  - Bot attempted Supabase inserts (bad).  
  - Render logs confirmed 404 for all 3 (bad).  
  - Supabase dashboard showed no new rows (bad).
- **Status:** ❌ Failed build — needs corrected REST path assembly and table name mapping.
- [2025-11-16] — Build 006 — GPT Prompt ID missing (model_not_found)

- Summary: After fixing Supabase path issues in Build 005, all parsing stopped working because the configured GPT_PROMPT_ID points to a deleted or invalid OpenAI prompt resource.
- Problem:
  - Render logs show: `model_not_found` for model `pmpt_690e9eb161048193a9e4d70bafc9608c0e5c9a3566d93187`.
  - This means the OpenAI Prompt Cache resource no longer exists.
  - As a result, `openai.responses.parse()` fails instantly and returns None.
  - The webhook goes into fallback → bot replies “Sorry, I could not process that.”
  - Supabase receives **zero** inserts.
- What changed:
  - No code change — the underlying OpenAI prompt ID became invalid.
  - All containers (food, sleep, exercise) failed equally.
- How it was tested:
  - Sent 3 logs (food → sleep → exercise).
  - Parser returned `None` each time.
  - Telegram output: “Sorry, I could not process that.”
  - Render logs: `Error code: 400 model_not_found`.
- Status: ❌ Failed — system cannot parse anything until a new PROMPT_ID is created and added to Render.

[2025-11-17] — Build 007 — GPT Responses API call failing (wrong request shape)

Summary:
Parser failed before container detection because the API call format to `client.responses.create()` did not match the required schema of the Responses API, causing immediate 400-level model errors.

Problem:
Render runtime logs showed repeated failures:
`GPT PARSE ERROR: model_not_found`  
and responses containing `"requested model does not exist"`.

This did NOT mean the prompt ID was wrong. The issue was:
- The request payload sent by main.py used fields that the Responses API does not accept.
- The `input=` block was formatted incorrectly or missing.
- The prompt attachment object was not invoked with the correct structure version.

As a result:
- GPT never produced a parsed container.
- No Supabase insert logic ran.
- Telegram only returned fallback messages.
- All test logs (food, sleep, exercise) failed at the same stage.

Changes:
None applied yet. This build only documents the failure and root cause.
Actual fixes will be included in Build 008.

How it was tested:
Sent three inputs through Telegram:
- Food log (“oats with protein powder”)
- Sleep log (“slept 7 hours, HR 55”)
- Exercise log (“5km run 30 mins”)

Every test produced GPT PARSE ERROR in Render logs and no Supabase insertion.

Status:
❌ Failed — Requires new build that corrects the Responses API request structure.

[2025-11-17] — Build 008 — JSON Parse Error Hotfix

Summary: Bot regained connection to parser, but ingestion now fails with "name 'null' is not defined" during JSON parse.

Problem:

OpenAI returned "null" (valid JSON),

but Python tries to eval() it somewhere,

or our JSON loader was replaced with a Python eval unsafe parse,

so "null" becomes an undefined Python symbol → crash.

What changed (root cause):

In Build 006/007 parts of the parser-handling block were rewritten.

Somewhere "json.loads(...)" was swapped for something that tries to interpret the string as Python notation (eval, or missing json= parameter in the response).

Because JSON uses null but Python uses None, Python throws:
"name 'null' is not defined".

How it was tested:

User sent a food → sleep → exercise message.

Telegram bot responded (“Sorry, I could not process that.”), proving parser responded but JSON parsing failed.

Render logs show multiple crashes with identical signature.

Status: ❌ Failed build — needs enforced json.loads() for all parsing paths + validation of returned structure.

[2025-11-19] — Build 009 — Correct Supabase Column Types (Food/Sleep/Exercise)

Summary:
Converted all user-input columns from TEXT → proper numeric/timestamp types (INTEGER, DOUBLE PRECISION, TIMESTAMPTZ).

Problem:
Supabase inserts were failing with 404/400 errors because numbers were being sent as numeric, but the database was expecting TEXT. This mismatch broke the pipeline and caused all inserts to fail.

What we changed:
- Updated food table: calories, protein_g, carbs_g, fat_g, fiber_g → DOUBLE PRECISION.
- Updated sleep table: scores → INTEGER, duration_hr → DOUBLE, HR → INTEGER, timestamps → TIMESTAMPTZ.
- Updated exercise table: numeric performance fields → INTEGER/DOUBLE.
- Left system columns untouched.

How tested:
After migration, schema accepted manual inserts of numeric values without type errors (verified inside SQL editor).

Status: ✅ Stable — backend ready for new main.py build.

[2025-11-19] – Build 010 – Critical Fix: Wrong Supabase URL

Issue:
All inserts for food/sleep/exercise failed with Supabase error PGRST125 ("Invalid path specified in request URL"). Render logs showed correct JSON, correct rows, correct API key — but wrong Supabase URL.

Cause:
SUPABASE_URL environment variable was mistakenly set to the Supabase *dashboard* URL instead of the Supabase *project REST endpoint* URL.

Fix:
Retrieve the real project API URL:
Supabase → Project Settings → API → "Project URL"
Set SUPABASE_URL = that value (ending in `.supabase.co`)
Do NOT include `/rest/v1` (code appends it automatically).

Status:
Blocking issue resolved. Ready for retest once URL updated.

CHANGELOG 011 — User ID Removal + Schema Cleanup + Successful Insert Ops
Date: 2025-11-19
Build: 011
Status: Successful

1) user_id removed entirely from food, sleep, and exercise tables.
Reason:
- user_id caused UUID mismatch errors
- foreign key conflicts
- invalid input syntax failures
- redundant because chat_id is already unique per user

Action:
- Dropped user_id from all three tables
- Removed all user_id usage from the Python code
- All inserts now only use chat_id + date + container fields

Result:
- No more UUID errors
- No constraint failures
- Clean inserts across all containers

2) Standardized all numeric column types

FOOD TABLE TYPES:
- calories: float8
- protein_g: float8
- carbs_g: float8
- fat_g: float8
- fiber_g: float8
- meal_name: text
- notes: text
- chat_id: text
- date: date

SLEEP TABLE TYPES:
- sleep_score: int
- energy_score: int
- duration_hr: float8
- resting_hr: int
- sleep_start: timestamp
- sleep_end: timestamp
- notes: text
- chat_id: text
- date: date

EXERCISE TABLE TYPES:
- workout_name: text
- distance_km: float8
- duration_min: int
- calories_burned: int
- training_intensity: int
- avg_hr: int
- max_hr: int
- training_type: text
- perceived_intensity: int
- effort_description: text
- tags: text
- notes: text
- chat_id: text
- date: date

Result:
- All numerical fields accept proper numerical input
- No conversion errors

3) Code adjustments
- Removed all references to user_id
- Updated insert payloads to match new schema
- Only writes fields that exist in DB
- All containers now write using chat_id and date

4) Live test confirmation
Food insert: SUCCESS
Sleep insert: SUCCESS
Exercise insert: SUCCESS

All rows inserted correctly with:
chat_id = 2052083060
date = 2025-11-19

5) Build summary
- user_id removed permanently
- schema cleaned and normalized
- number types fixed
- main.py updated
- all insert operations functioning as expected

Build 011 marked as stable.

12. CHANGELOG — Build 012 (20/11/2025)
Status: Completed
Summary: Core Architecture Refactor — Legacy Module Isolation + API Bootstrap Integration

12.1 Overview
This build isolates all legacy ingestion modules, introduces the new API blueprint architecture, and prepares the system for the upcoming unified “Container Engine v2.” The refactor was designed to remove cross-module coupling, eliminate import ambiguity, and create a scalable foundation for future ingestion, classification, and validation pipelines.

12.2 Folder Refactor: Legacy Isolation
The previous folder layout contained legacy ingestion code inside `app/clients` and `app/processors`, which mixed transport responsibilities (Telegram handler) with parsing and transformation logic.  
To prevent conflicts with the new modular parser and blueprint system, both folders were migrated to legacy holding zones.

Changes:
• `app/clients` → `app/clients_legacy`
• `app/processors` → `app/processors_legacy`

Rationale:
• Eliminates namespace collisions during parser engine expansion.
• Removes ambiguity for future imports (especially for GPT-driven ingestion).
• Maintains backward compatibility by retaining legacy files without executing them.

Impact:
• No runtime imports depend on these folders anymore.
• No breaking changes to current ingestion behavior since legacy modules were already inactive.

12.3 API Bootstrap Integration
The new architecture introduces a clear separation of concerns via Flask Blueprints.

Added:
• `app/api/` directory
• `app/api/webhook.py` (blueprint-based Telegram webhook)

Blueprint Purpose:
• Encapsulates routing logic separate from application root.
• Ensures route handlers remain fully decoupled from ingestion and parsing engines.
• Enables future expansion (multiple endpoints: voice_ingest, screenshot_ingest, admin_ping, healthcheck, container_sync).

12.4 Parser Engine Routing (Engine v1 Bridge)
The `parser/engine.py` now acts as the central router between:
• raw Telegram input  
• GPT classification output  
• Supabase persistence  

This aligns with the planned “Container Engine v2” where:
• ingestion → classification → shape → validation → dispatch → persistence  
becomes a strict pipeline.

12.5 Import Health & Runtime Safety
After the refactor, all imports were validated to ensure no broken paths remain.

Confirmed:
• `main.py` imports only `from app.api.webhook import api` (correct)
• `parser.engine` remains intact and operational
• Supabase client initialization unaffected
• No circular imports introduced
• No reference exists to `clients` or `processors` after migration

Runtime Behavior:
• Application boots without error.
• Telegram webhook responds correctly.
• Unknown messages correctly fall back to the "unknown" classification path.

12.6 Deployment Impact
Deployment confirmed via Render logs:
• App runs with blueprint correctly registered.
• No import errors.
• Legacy folders ignored as intended.

This build produces a stable surface for Step 7 (Container Engine v2 introduction).

12.7 Next Required Engineering Steps
• Replace legacy ingestion routines with the new modular parser pipeline.
• Move Supabase logic to `services/supabase.py` and centralize DB IO.
• Introduce unified validation layer for GPT output.
• Expand blueprint to handle multi-modal ingest (voice, screenshot, text).
• Introduce request ID tracing for debugging pipeline execution.

End of Build 012.

### [2025-11-20] — Build 013 — Container Engine v2 Initialization (Structural Refactor)

**Summary:**  
Major architectural restructuring to prepare the system for multi-modal ingestion (text, photos, voice) and future Container Engine v2. Introduced modular folder structure, blueprint routing, service layer abstraction, and global shadow-logging.

**Problem Being Solved:**  
The previous monolithic `main.py` made the system fragile and unable to scale to image and audio ingestion. No separation of concerns between parser logic, Telegram logic, Supabase logic, and routing. No reliable logging mechanism for future AI training or debugging.

**What Changed:**  
- Added `app/api/` with Blueprint routing (`webhook.py`)  
- Added `app/services/` for all Supabase + Telegram operations  
- Added `app/parser/` for GPT parsing engine (modular)  
- Added `app/utils/time.py` for clean date handling  
- Implemented shadow-logging into `public.entries`  
- Refactored ingestion flow to route through isolated modules  
- Removed legacy monolithic code structure  
- Added future-proof hooks for Router, OCR, and ASR modules  
- Unified logging across all modules for debugging consistency  

**How It Was Tested:**  
- Sent unknown message (“blim blim”) → classified as unknown  
- Shadow-log successfully inserted into `public.entries` with all fields  
- Realtime subscription showed correct payload in Supabase UI  
- Sent valid food log → correct routing to `public.food`  
- System stable (200 OK responses, no crashes, no PGRST errors)  

**Status:**  
✅ Stable — Architecture foundation complete and production-ready  



---


